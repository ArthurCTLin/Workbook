{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimCLR.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNdFCrk6FiVxHp5OzUQnlq7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArthurCTLin/Workbook/blob/main/SimCLR/SimCLR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-supervised Learning with SimCLR (Pytorch)"
      ],
      "metadata": {
        "id": "5bRXEozRBsZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paper Concept\n",
        "* **Showed the importance of composition of data augmentation :**\n",
        "    * The composition of mulitple data augmentation operation can enhance up the performance constrative prediction task which try to obtain more useful representations.\n",
        "    * Unsupervised contrastive learning benefits from stronger data augmentation than supervised learning.\n",
        "* **Introduced a trainable nonlinear MLP structure ( $g(*)$ in the left below figure ) between representation and constative loss to improve the quality of representation.**\n",
        "* **Normalized embeddings** and an appropriately adjusted **temperature parameter** can enhance up the performance of representation learning with contrastive cross entropy loss.\n",
        "* **Contrastive learning benefits from larger batch sizes and more training steps/epochs.**\n",
        "<img src=\"https://i.imgur.com/hhSyTPd.png\" width=50%><img src=\"https://i.imgur.com/G95735O.gif\" width=50%>\n",
        "\n",
        "$\\qquad \\qquad \\qquad \\quad \\quad \\qquad$**(Figure Source: [Paper](https://arxiv.org/pdf/2002.05709.pdf))** $\\quad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\quad \\qquad$**(Figure Source: [SimCLR Github](https://github.com/google-research/simclr))**"
      ],
      "metadata": {
        "id": "-AL7pnFyDBp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Rf2xqkhlCchK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mjz9BZHBpuA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil, time, os, requests, random, copy\n",
        "\n",
        "# Torch or Torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.datasets import STL10\n",
        "from torchvision.datasets import DatasetFolder\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Reproduction\n",
        "myseed = 42069\n",
        "torch.backends.cudnn.deterministic = True \n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(myseed)\n",
        "torch.manual_seed(myseed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(myseed)\n",
        "    \n",
        "# Device\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device: \", device)\n",
        "print(\"Number of workers: \", NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformation\n",
        "As mentioned in contributions, the composition of mulitple data augmentation operation can enhance up the performance of SimCLR. Therefore, the following operations are applied:\n",
        "* [**ColorJitter:**](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html) brightness, contrast, saturation, hue \n",
        "* [**RandomResizedCrop**](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html#torchvision.transforms.RandomResizedCrop)\n",
        "* [**RandomHorizontalFlip**](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomHorizontalFlip.html#torchvision.transforms.RandomHorizontalFlip)\n",
        "* [**RandomGrayscale**](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomGrayscale.html#torchvision.transforms.RandomGrayscale)\n",
        "\n",
        "$\\qquad \\qquad \\qquad \\qquad$![](https://i.imgur.com/w6WVWDj.png)"
      ],
      "metadata": {
        "id": "KJKa_OsUEGi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformsSimCLR:\n",
        "    \"\"\"\n",
        "    A stochastic data augmentation module that transforms any given data example randomly\n",
        "    resulting in two correlated views of the same example,\n",
        "    denoted x ̃i and x ̃j, which we consider as a positive pair.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, s=1, prob=0.8):\n",
        "        color_jitter = torchvision.transforms.ColorJitter(\n",
        "            0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s\n",
        "        )\n",
        "        self.train_transform = torchvision.transforms.Compose(\n",
        "            [\n",
        "                torchvision.transforms.RandomResizedCrop(size=size),\n",
        "                torchvision.transforms.RandomHorizontalFlip(),  # with 0.5 probability\n",
        "                torchvision.transforms.RandomApply([color_jitter], p=prob),\n",
        "                torchvision.transforms.RandomGrayscale(p=0.25*prob),\n",
        "                transforms.GaussianBlur(kernel_size=9),\n",
        "                torchvision.transforms.ToTensor(),\n",
        "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        return self.train_transform(x), self.train_transform(x)\n"
      ],
      "metadata": {
        "id": "xM3tUWxrEFIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset\n",
        "\n",
        "* **STL-10 dataset** is an image recognition dataset for developing unsupervised feature learning, deep learning, self-taught learning algorithms. \n",
        "* **10 classes:** airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck.\n",
        "* **Components:**\n",
        "  * Training: 5000 images (500 per class and split into 10 pre-defined folds)\n",
        "  * Testing: 8000 images (800 per class)\n",
        "  * Unlabeled: 100000 images for unsupervised learning."
      ],
      "metadata": {
        "id": "X5ldyiFME8aX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DATASET_PATH = './'\n",
        "unlabeled_data = STL10(root=DATASET_PATH, split='unlabeled', download=True,\n",
        "              transform=TransformsSimCLR(size=96, s=1, prob=0.8))\n",
        "train_data_contrast = STL10(root=DATASET_PATH, split='train', download=True,\n",
        "                transform=TransformsSimCLR(size=96, s=1, prob=0.8))\n",
        "\n",
        "\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(unlabeled_data_total, batch_size=batch_size, shuffle=True,\n",
        "               drop_last=True, pin_memory=True, num_workers=NUM_WORKERS)\n",
        "valid_loader = DataLoader(train_data_contrast, batch_size=batch_size, shuffle=False,\n",
        "               drop_last=True, pin_memory=True, num_workers=NUM_WORKERS)"
      ],
      "metadata": {
        "id": "3GmqXYhFEfH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_IMAGES = 6\n",
        "imgs = torch.stack([img for idx in range(NUM_IMAGES) for img in unlabeled_data[idx][0]], dim=0)\n",
        "img_grid = torchvision.utils.make_grid(imgs, nrow=6, normalize=True, pad_value=0.9)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title('Augmented image examples of the STL10 dataset')\n",
        "plt.imshow(img_grid)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "tvPV_07iE2HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "* *Base encoder $f(*)$ :* ResNet-50\n",
        "* *Projection head $g(*) :$* 2-layer MLP projection with 128-dimensional latent space."
      ],
      "metadata": {
        "id": "fReu5QopFr-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimCLR(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder):\n",
        "        super(SimCLR, self).__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "\n",
        "        dim_mlp = self.encoder.fc.in_features\n",
        "        self.encoder.fc = nn.Sequential(nn.Linear(dim_mlp, dim_mlp),\n",
        "                          nn.ReLU(),\n",
        "                          self.encoder.fc\n",
        "                          )\n",
        "                    \n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)"
      ],
      "metadata": {
        "id": "ktylgK2zFyJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "projection_dim = 128\n",
        "encoder = models.resnet50(pretrained=False, num_classes=projection_dim)\n",
        "model = SimCLR(encoder).to(device)"
      ],
      "metadata": {
        "id": "tuK2TM_kGF1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss\n",
        "Randomly sample a batch of N examples and obtain 2N data points (as mentioned above, one example is transformed into two correlated views of the given image).\n",
        "* The pair from the same example is defined as **positive pair**.\n",
        "* **Loss function** of the positive pair is defined:\n",
        "$$l_{i, j} = -log \\frac{exp(sim(z_{i}, z_{j})/\\tau)}{\\sum^{2N}_{k=1} 1_{k\\neq i}exp(sim(z_{i}, z_{j})/\\tau)}$$\n",
        "  * $sim(x, y)$ is the consine similarity. $sim(x, y)=\\frac{x^{T}y}{||x|| ||y||}$.\n",
        "  * $1_{k\\neq i}$ : is an indicator function evaluating to 1 iff $k\\neq i$.\n",
        "  * $\\tau$ : temperature parameter.\n",
        "  * The loss is based on **Infomation Noise Contrastive Estimation Loss (InfoNCE Loss)**."
      ],
      "metadata": {
        "id": "la2PXCdWGe6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def info_nce_loss(features):\n",
        "\n",
        "        labels = torch.cat([torch.arange(batch_size) for i in range(2)], dim=0)\n",
        "        labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        features = F.normalize(features, dim=1)\n",
        "\n",
        "        similarity_matrix = torch.matmul(features, features.T)\n",
        "        \n",
        "        mask = torch.eye(labels.shape[0], dtype=torch.bool).to(device)\n",
        "        labels = labels[~mask].view(labels.shape[0], -1)\n",
        "        similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)\n",
        "        \n",
        "        positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1)\n",
        "\n",
        "        negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1)\n",
        "\n",
        "        logits = torch.cat([positives, negatives], dim=1)\n",
        "        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(device)\n",
        "\n",
        "        logits = logits / 0.5 #temperature parameter\n",
        "        return logits, labels"
      ],
      "metadata": {
        "id": "2K61cBcNGSM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer (Optional)\n",
        "* **Layer-wise Adaptive Rate Scaling (LARS)** \n",
        "* This optimizer is more suitable to the training with large batch size \n",
        "* The source paper: [Large batch training of convolutional networks](https://arxiv.org/abs/1708.03888)\n",
        "* ***However, due to the limited memory in kaggle, 256 is the largest batch size I can set. Therefore, this technique is not adopted in the implementation.***"
      ],
      "metadata": {
        "id": "RAjC_pEjJmwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.optimizer import Optimizer, required\n",
        "import re\n",
        "\n",
        "EETA_DEFAULT = 0.001\n",
        "\n",
        "\n",
        "class LARS(Optimizer):\n",
        "    \"\"\"\n",
        "    Layer-wise Adaptive Rate Scaling for large batch training.\n",
        "    Introduced by \"Large Batch Training of Convolutional Networks\" by Y. You,\n",
        "    I. Gitman, and B. Ginsburg. (https://arxiv.org/abs/1708.03888)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        params,\n",
        "        lr=required,\n",
        "        momentum=0.9,\n",
        "        use_nesterov=False,\n",
        "        weight_decay=0.0,\n",
        "        exclude_from_weight_decay=None,\n",
        "        exclude_from_layer_adaptation=None,\n",
        "        classic_momentum=True,\n",
        "        eeta=EETA_DEFAULT,\n",
        "    ):\n",
        "        \"\"\"Constructs a LARSOptimizer.\n",
        "        Args:\n",
        "        lr: A `float` for learning rate.\n",
        "        momentum: A `float` for momentum.\n",
        "        use_nesterov: A 'Boolean' for whether to use nesterov momentum.\n",
        "        weight_decay: A `float` for weight decay.\n",
        "        exclude_from_weight_decay: A list of `string` for variable screening, if\n",
        "            any of the string appears in a variable's name, the variable will be\n",
        "            excluded for computing weight decay. For example, one could specify\n",
        "            the list like ['batch_normalization', 'bias'] to exclude BN and bias\n",
        "            from weight decay.\n",
        "        exclude_from_layer_adaptation: Similar to exclude_from_weight_decay, but\n",
        "            for layer adaptation. If it is None, it will be defaulted the same as\n",
        "            exclude_from_weight_decay.\n",
        "        classic_momentum: A `boolean` for whether to use classic (or popular)\n",
        "            momentum. The learning rate is applied during momeuntum update in\n",
        "            classic momentum, but after momentum for popular momentum.\n",
        "        eeta: A `float` for scaling of learning rate when computing trust ratio.\n",
        "        name: The name for the scope.\n",
        "        \"\"\"\n",
        "\n",
        "        self.epoch = 0\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            momentum=momentum,\n",
        "            use_nesterov=use_nesterov,\n",
        "            weight_decay=weight_decay,\n",
        "            exclude_from_weight_decay=exclude_from_weight_decay,\n",
        "            exclude_from_layer_adaptation=exclude_from_layer_adaptation,\n",
        "            classic_momentum=classic_momentum,\n",
        "            eeta=eeta,\n",
        "        )\n",
        "\n",
        "        super(LARS, self).__init__(params, defaults)\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.weight_decay = weight_decay\n",
        "        self.use_nesterov = use_nesterov\n",
        "        self.classic_momentum = classic_momentum\n",
        "        self.eeta = eeta\n",
        "        self.exclude_from_weight_decay = exclude_from_weight_decay\n",
        "        # exclude_from_layer_adaptation is set to exclude_from_weight_decay if the\n",
        "        # arg is None.\n",
        "        if exclude_from_layer_adaptation:\n",
        "            self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n",
        "        else:\n",
        "            self.exclude_from_layer_adaptation = exclude_from_weight_decay\n",
        "\n",
        "    def step(self, epoch=None, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        if epoch is None:\n",
        "            epoch = self.epoch\n",
        "            self.epoch += 1\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            weight_decay = group[\"weight_decay\"]\n",
        "            momentum = group[\"momentum\"]\n",
        "            eeta = group[\"eeta\"]\n",
        "            lr = group[\"lr\"]\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                param = p.data\n",
        "                grad = p.grad.data\n",
        "\n",
        "                param_state = self.state[p]\n",
        "\n",
        "                # TODO: get param names\n",
        "                # if self._use_weight_decay(param_name):\n",
        "                grad += self.weight_decay * param\n",
        "\n",
        "                if self.classic_momentum:\n",
        "                    trust_ratio = 1.0\n",
        "\n",
        "                    # TODO: get param names\n",
        "                    # if self._do_layer_adaptation(param_name):\n",
        "                    w_norm = torch.norm(param)\n",
        "                    g_norm = torch.norm(grad)\n",
        "\n",
        "                    device = g_norm.get_device()\n",
        "                    trust_ratio = torch.where(\n",
        "                        w_norm.gt(0),\n",
        "                        torch.where(\n",
        "                            g_norm.gt(0),\n",
        "                            (self.eeta * w_norm / g_norm),\n",
        "                            torch.Tensor([1.0]).to(device),\n",
        "                        ),\n",
        "                        torch.Tensor([1.0]).to(device),\n",
        "                    ).item()\n",
        "\n",
        "                    scaled_lr = lr * trust_ratio\n",
        "                    if \"momentum_buffer\" not in param_state:\n",
        "                        next_v = param_state[\"momentum_buffer\"] = torch.zeros_like(\n",
        "                            p.data\n",
        "                        )\n",
        "                    else:\n",
        "                        next_v = param_state[\"momentum_buffer\"]\n",
        "\n",
        "                    next_v.mul_(momentum).add_(scaled_lr, grad)\n",
        "                    if self.use_nesterov:\n",
        "                        update = (self.momentum * next_v) + (scaled_lr * grad)\n",
        "                    else:\n",
        "                        update = next_v\n",
        "\n",
        "                    p.data.add_(-update)\n",
        "                else:\n",
        "                    raise NotImplementedError\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def _use_weight_decay(self, param_name):\n",
        "        \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
        "        if not self.weight_decay:\n",
        "            return False\n",
        "        if self.exclude_from_weight_decay:\n",
        "            for r in self.exclude_from_weight_decay:\n",
        "                if re.search(r, param_name) is not None:\n",
        "                    return False\n",
        "        return True\n",
        "\n",
        "    def _do_layer_adaptation(self, param_name):\n",
        "        \"\"\"Whether to do layer-wise learning rate adaptation for `param_name`.\"\"\"\n",
        "        if self.exclude_from_layer_adaptation:\n",
        "            for r in self.exclude_from_layer_adaptation:\n",
        "                if re.search(r, param_name) is not None:\n",
        "                    return False\n",
        "        return True"
      ],
      "metadata": {
        "id": "-I6XNL5jJl79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer, Scheduler and Loss Function Declaration"
      ],
      "metadata": {
        "id": "rLKSRhOyL5NS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#OPTMIZER\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
        "\n",
        "# \"decay the learning rate with the cosine decay schedule without restarts\"\n",
        "#SCHEDULER OR LINEAR EWARMUP\n",
        "warmupscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch : (epoch+1)/10.0, verbose = True)\n",
        "\n",
        "#SCHEDULER FOR COSINE DECAY\n",
        "mainscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 500, eta_min=0.05, last_epoch=-1, verbose = True)\n",
        "\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
        "                               T_max=len(train_loader), \n",
        "                               eta_min=0,\n",
        "                               last_epoch=-1)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "k256wEqpL69q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "Since the limited running time (12 hrs) and memory of kaggle gpu resource. I just set the number of training epoch as 50. The result with more epochs is conducted with resuming learning and will be demonstrated afterwards."
      ],
      "metadata": {
        "id": "7WPQlXXwMZQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"model.ckpt\"\n",
        "\n",
        "nr = 0\n",
        "current_epoch = 0\n",
        "epochs = 50\n",
        "tr_loss = []\n",
        "val_loss = []\n",
        "\n",
        "best_loss = 9999.0\n",
        "train_loss_report = 0.0\n",
        "valid_loss_report = 0.0\n",
        "train_loss_record = []\n",
        "valid_loss_record = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "        \n",
        "    print(f\"Epoch [{epoch}/{epochs}]\\t\")\n",
        "    stime = time.time()\n",
        "\n",
        "    model.train()\n",
        "    tr_loss_epoch = 0\n",
        "    train_loss = []\n",
        "    \n",
        "    for batch in tqdm(train_loader):\n",
        "        imgs, _ = batch\n",
        "        imgs = torch.cat(imgs, dim=0).to(device)\n",
        "        \n",
        "        features = model(imgs)\n",
        "        logits, labels = info_nce_loss(features)\n",
        "        loss = criterion(logits, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "    \n",
        "        tr_loss_epoch += loss.item()\n",
        "        train_loss.append(loss.item())\n",
        "        \n",
        "    train_loss_report = sum(train_loss) / len(train_loss)\n",
        "    print(f\"[ Train | {epoch + 1:03d}/{epochs:03d} ] loss = {train_loss_report:.5f}\")\n",
        "    \n",
        "    if epoch >= 10:\n",
        "        scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss_epoch = 0\n",
        "        valid_loss = []\n",
        "        for batch in tqdm(valid_loader):\n",
        "\n",
        "            imgs, _ = batch\n",
        "            imgs = torch.cat(imgs, dim=0).to(device)\n",
        "        \n",
        "            features = model(imgs)\n",
        "            logits, labels = info_nce_loss(features)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            val_loss_epoch += loss.item()\n",
        "            valid_loss.append(loss.item())\n",
        "        \n",
        "    valid_loss_report = sum(valid_loss) / len(valid_loss)\n",
        "    print(f\"[ Valid | {epoch + 1:03d}/{epochs:03d} ] loss = {valid_loss_report:.5f}\")\n",
        "\n",
        "    time_taken = (time.time()-stime)/60\n",
        "    print(f\"Epoch [{epoch}/{epochs}]\\t Time Taken: {time_taken} minutes\")\n",
        "    \n",
        "    if valid_loss_report < best_loss:\n",
        "        best_loss = valid_loss_report\n",
        "        torch.save({'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': loss}, \n",
        "                    model_path)\n",
        "        print(\"The model is save!\")\n",
        "    \n",
        "    train_loss_record.append(train_loss_report)\n",
        "    valid_loss_record.append(valid_loss_report)"
      ],
      "metadata": {
        "id": "MjOSGqzhMPCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting"
      ],
      "metadata": {
        "id": "QtE8BLr1M19P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.arange(len(train_loss_record))\n",
        "plt.plot(x, train_loss_record, color=\"blue\", label=\"Train\")\n",
        "plt.plot(x, valid_loss_record, color=\"red\", label=\"Valid\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jQNNvwnFMy7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result\n",
        "$\\qquad \\qquad \\qquad \\qquad$<img src=\"https://i.imgur.com/46oniYI.png\" width=50%>\n",
        "\n",
        "$\\qquad\\quad \\qquad \\quad \\quad\\qquad \\qquad \\qquad \\qquad \\quad$ **The loss of training and validation**"
      ],
      "metadata": {
        "id": "L8phiPplM-aL"
      }
    }
  ]
}